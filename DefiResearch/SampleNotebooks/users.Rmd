---
title: "DeFi Example Notebook: AAVE Users"
subtitle: "AAVE Users Analysis"
author: "Data Analytics Research Instructors"
date: "08/27/2021"
output:
  html_document:
    toc: true
    number_sections: true
    df_print: paged
  pdf_document: default
---


```{r setup, include=FALSE}
# Set the default CRAN repository
local({r <- getOption("repos")
       r["CRAN"] <- "http://cran.r-project.org" 
       options(repos=r)
})

# Set code chunk defaults
knitr::opts_chunk$set(echo = TRUE)


# Load required packages; install if necessary
# CAUTION: DO NOT interrupt R as it installs packages!!
if (!require("ggplot2")) {
  install.packages("ggplot2")
  library(ggplot2)
}

if (!require("gplots")) {
  install.packages("gplots")
  library(gplots)
}

if (!require("RColorBrewer")) {
  install.packages("RColorBrewer")
  library(RColorBrewer)
}
if (!require("beeswarm")) {
  install.packages("beeswarm")
  library(beeswarm)
}
if (!require("tidyverse")) {
  install.packages("tidyverse")
  library(tidyverse)
}
if (!require("ggbeeswarm")) {
  install.packages("ggbeeswarm")
  library(ggbeeswarm)
}
if (!require("xts")) {
  install.packages("xts")
  library(xts)
}
if (!require("foreach")) {
  install.packages("foreach")
  library(foreach)
}
if (!require("doParallel")) {
  install.packages("doParallel")
  library(doParallel)
}


```

# DeFi User Analysis

## Prepare Transaction Data

We begin by loading our prepared AAVE transaction data into a dataframe. The dataset has over 400,000 rows, and 27 columns. 

We are directly loading the dataframe from an Rds archive instead of a CSV file to conserve space. 

```{r}
#load Rds (binary version of csv file) into dataframe
df<-read_rds('../Data/transactions.Rds')

# Let's take a quick look 
head(df)

```

We reformat the dataframe so that each row represents a user, and the columns represent a summarization of the user's transaction history. 

```{r}
#group by user and get time of user's first and last transaction, as well as number of transactions
df.users<- df%>%group_by(user)%>%
  summarise(timefirst=min(timestamp), timelast=max(timestamp), N=n())

#get the time the user has been active
df.users$timeactive<-df.users$timelast-df.users$timefirst

#get user's transaction information
for(Type in c(unique(df$type))){
  #filter for only transactions of certain type
  df.type <-filter(df%>%group_by(user)%>%
                     count(type),type==Type)
  
  #add means of each transaction type
  if(Type!="liquidation" || Type!="swap"){
    df.mean<-filter(df,type==Type)%>%
      group_by(user)%>%
      summarise(Mean=mean(amountUSD))
    colnames(df.mean)[2]<-paste('mean_',Type,sep='')
    df.users<-merge(x=df.users,y=df.mean,by="user",all.x=TRUE)
  }
  
  #add counts of transaction types to df
  ntypes<-paste("n",Type,sep='')
  colnames(df.type)[3]<-ntypes
  df.users<-merge(x=df.users,y=select(df.type,user,ntypes),by="user",all.x=TRUE)
  
  #get proportion of transaction types and weekly number of transaction type
  df.users[paste("prop_",Type,sep='')]<-df.users[ntypes]/((df.users$N)+1)
  df.users[paste("weekly_",Type,sep='')]<-df.users[ntypes]/(((df.users$timeactive)+1)/(3600*24*7))
}

df.users
```
Next, we select only the columns we wish to cluster on.

```{r}
#subset only columns we wish to scale
df.sub<-select(df.users,-c(user,timefirst,timelast,mean_swap,mean_liquidation))

#replce misisng values as 0's
df.sub<-df.sub%>%replace(is.na(.),0)
```

We scale the data to prepare for PCA.

```{r}
#scale data
df.scaled<-df.sub%>%mutate_all(scale)
df.scaled
```

Now, we perform PCA on the scaled data. 

```{r}
#perform pca on data
my.pca<-prcomp(df.scaled,retx=TRUE) # Run PCA and save to my.pca

#make scree plot
plot(my.pca, type="line")

#summary of pca
summary(my.pca)

ncomps=5
```
Next, we plot a heatmap to show the makeups of the principal components. 

```{r}
V <- t(my.pca$rotation[,1:ncomps]) # We transpose to make the principal components be rows
heatmap.2(V, main='Principal Components', cexRow=0.75, cexCol=0.75, scale="none", dendrogram="none",
Colv= FALSE, Rowv=FALSE, tracecol=NA,density.info='none')

```
Finally, we run the kmeans clustering algorithm on the data. We select the number of clusters euqal to the number of selected components. 

```{r}
#run kmeans algorithm
pca.matrix<-my.pca$x[,1:ncomps]
set.seed(1)
km <-kmeans(pca.matrix, ncomps)

#plot frequencies of each cluster
barplot(table(km$cluster),main="Cluster Frequencies")
```
Finally, we view the centers of the kmeans clusters.

```{r}
#make heatmap of cluster centers
heatmap.2(km$centers,
scale = "none",
dendrogram = "none",
Colv=FALSE,
cexCol=1.0,
main = "Kmeans Cluster Centers",
trace ="none")
```

